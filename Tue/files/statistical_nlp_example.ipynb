{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2015cda",
   "metadata": {},
   "source": [
    "<b>Simple script to train Hidden Markov Model for Part of Speech tagging using NLTK</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852597d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the models\n",
    "import nltk\n",
    "from nltk import HiddenMarkovModelTagger as hmm # do not use nltk.tag.hmm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "import warnings\n",
    "import dill\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5f5f7",
   "metadata": {},
   "source": [
    "<b>Download the data. Run only once</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the treebank dataset is downloaded\n",
    "#nltk.download('treebank')\n",
    "#nltk,download('punkt')\n",
    "#nltk,download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7054773e",
   "metadata": {},
   "source": [
    "<b>Prepare the data. We'll use the Brown which is an English Corpus that includes pos tagging. We split the data into training and testing. Try to change the data size and experiment with the accuracy change.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985fc10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of tagged examples in the dataset is: {len(brown.tagged_sents(tagset=\"universal\"))}')\n",
    "train_data = brown.tagged_sents(tagset='universal')[:50000]\n",
    "test_data = brown.tagged_sents(tagset='universal')[50000:]\n",
    "\n",
    "print(f'len of training data is {len(train_data)}')\n",
    "print(f'len of testing data is {len(test_data)}')\n",
    "print(train_data[0])\n",
    "\n",
    "# Extracting unique tags from train_data\n",
    "unique_tags = set(tag for sent in train_data for _, tag in sent)\n",
    "\n",
    "print(unique_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896747b",
   "metadata": {},
   "source": [
    "<b>Define the trainer and train the model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a52aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = hmm.train(train_data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's accuracy on the test data\n",
    "accuracy = tagger.accuracy(test_data)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7189e2",
   "metadata": {},
   "source": [
    "<b>Generate true tags list and model prediction to get more detailed stats on where the model performed better and where it didn't perform so well</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5228907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Predictions\n",
    "true_tags = [tag for sent in test_data for _, tag in sent]\n",
    "predicted_tags = [tag for sent in tagger.tag_sents([[word for word, _ in sent] for sent in test_data]) for _, tag in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a658f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy for each label\n",
    "labels = list(set(true_tags))\n",
    "for label in labels:\n",
    "    correct_predictions = sum(1 for t, p in zip(true_tags, predicted_tags) if t == label and p == label)\n",
    "    total_predictions = sum(1 for t in true_tags if t == label)\n",
    "    wrong_predictions = total_predictions - correct_predictions\n",
    "    label_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Correct Predictions: {correct_predictions}\")\n",
    "    print(f\"Wrong Predictions: {wrong_predictions}\")\n",
    "    print(f\"Accuracy: {label_accuracy:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd1f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_tags, predicted_tags, labels=labels)\n",
    "\n",
    "# To make the confusion matrix more readable, you can use a DataFrame\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=labels, columns=labels)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_df)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7d6ea",
   "metadata": {},
   "source": [
    "<b>If I'm happy with the model, I can save it for later usage</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "with open('hmm_tagger.pkl', 'wb') as f:\n",
    "    dill.dump(tagger, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e01b2a",
   "metadata": {},
   "source": [
    "<b>You can load the model at anytime to use it for tagging sentences</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model from the file\n",
    "with open('hmm_tagger.pkl', 'rb') as f:\n",
    "    loaded_tagger = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0dc08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'I took the train from Zurich to Italy last night'\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Tag the tokenized sentence\n",
    "tagged_sentence = loaded_tagger.tag(tokens)\n",
    "\n",
    "print(tagged_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d7c715",
   "metadata": {},
   "source": [
    "<b> Let us try another model for the same task. How about CRF (Conditional Random Forest)? Will it perform better in a token classifcation task?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af434b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import metrics\n",
    "import sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efe3173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7358e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(s) for s in train_data]\n",
    "y_train = [sent2labels(s) for s in train_data]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_data]\n",
    "y_test = [sent2labels(s) for s in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1db55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31303e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "try:\n",
    "    crf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97308ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9242331",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"crf_model.pkl\", \"wb\") as out_file:\n",
    "    dill.dump(crf, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb0b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"crf_model.pkl\", \"rb\") as in_file:\n",
    "    crf = dill.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a606b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(sentence):\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "    # Convert the words into features\n",
    "    features = sent2features([(word, None) for word in words])\n",
    "    # Predict the tags\n",
    "    tags = crf.predict_single(features)\n",
    "    # Return the words with their predicted tags\n",
    "    return list(zip(words, tags))\n",
    "\n",
    "# Example usage\n",
    "sentence = \"This is a test sentence.\"\n",
    "print(pos_tag(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee59eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "\n",
    "\n",
    "# Flatten the test and predicted labels lists\n",
    "y_test_flat = [label for sentence in y_test for label in sentence]\n",
    "y_pred_flat = [label for sentence in y_pred for label in sentence]\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_flat, y_pred_flat, labels=list(unique_tags))\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=list(unique_tags), yticklabels=list(unique_tags))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c477e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
