{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<p>\n",
        "Bern Winter School - Natural Language Processing <br>\n",
        "Data Science Lab, University of Bern, 2024 <br>\n",
        "Prepared by Dr. Mykhailo Vladymyrov.\n",
        "\n",
        "</p>\n",
        "\n",
        "This work is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
      ],
      "metadata": {
        "id": "h_875srK9_2I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SFDHxitoA-5"
      },
      "source": [
        "# Install libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EQh61dvDCp0"
      },
      "outputs": [],
      "source": [
        "!pip install nltk spacy scikit-learn gensim matplotlib seaborn pandas tqdm flair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1nUMIbUoLl-"
      },
      "source": [
        "# Wed morning 1 (NN on tf-idf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this session we will continue with building simple neural networks.\n",
        "We will use the more sophisticated features, and rely on previously established intuition about building the NNs."
      ],
      "metadata": {
        "id": "mND6VjdqgoWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Imports"
      ],
      "metadata": {
        "id": "lwxYyCso5IFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "zoQhDMqOrtHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fetch dataset"
      ],
      "metadata": {
        "id": "2wajOn8m5Oq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_20newsgroups().target_names"
      ],
      "metadata": {
        "id": "D5xybPcrrcfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
        "\n",
        "We strip the headers and footers, as those can make the task easier."
      ],
      "metadata": {
        "id": "NL8T5Kb3r-nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the 20 Newsgroups dataset\n",
        "categories = ['rec.sport.hockey', 'sci.electronics', 'comp.graphics']\n",
        "SEED = 64\n",
        "train_data = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=SEED)\n",
        "test_data = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=SEED)"
      ],
      "metadata": {
        "id": "L_1c1ZY5sDwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the data structure\n",
        "train_data.keys()"
      ],
      "metadata": {
        "id": "LLNn0H4FsPBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the data input elements, target, target_names, number of elements... (5 min)\n"
      ],
      "metadata": {
        "id": "Gwj8tT7usQyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after add the `remove=('headers', 'footers', 'quotes')` argument to the `fetch_20newsgroups` call and repeat\n",
        "# what changed?"
      ],
      "metadata": {
        "id": "bAf5J1P6Z0Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Preprocess and inspect dataset"
      ],
      "metadata": {
        "id": "rP4-PTgK5jbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the train_data and test_data\n",
        "for d in [train_data, test_data]:\n",
        "\n",
        "  # Remove leading and trailing whitespace, tab,\n",
        "  # and new line characters from each data point\n",
        "  d.data = [s.strip(' \\n\\t\\r') for s in d.data]\n",
        "\n",
        "  # Get the indices of data points with non-empty content\n",
        "  ok_idx = [i for i, s in enumerate(d.data) if len(s) > 0]\n",
        "\n",
        "  # Filter the data and target lists to keep only non-empty data points\n",
        "  d.data = [d.data[i] for i in ok_idx]\n",
        "  d.target = [d.target[i] for i in ok_idx]"
      ],
      "metadata": {
        "id": "8KOXWnZ-vH2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the data labels elements\n",
        "#train_data.target"
      ],
      "metadata": {
        "id": "kmPukYChsebf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data.data), len(test_data.data)"
      ],
      "metadata": {
        "id": "3wtbKp2csr-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = train_data.data[24]\n",
        "sample_label = train_data.target[24]\n"
      ],
      "metadata": {
        "id": "hNJ87Mmksy3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample)\n",
        "print(f'label={sample_label}, ({categories[sample_label]})')"
      ],
      "metadata": {
        "id": "aveCoShCs0wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
      ],
      "metadata": {
        "id": "Z0floa6Fxw_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the text data to TF-IDF vectors, max 10 words\n",
        "vectorizer = TfidfVectorizer(max_features=10)\n",
        "\n",
        "# `object.fit` followed by processing by `object.transform` or the\n",
        "# joined `object.fit_transform` is the\n",
        "train_vectors = vectorizer.fit_transform(train_data.data[:3])"
      ],
      "metadata": {
        "id": "VUz-3pSW0C5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(len(feature_names))"
      ],
      "metadata": {
        "id": "h9yGUw5L0dm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names"
      ],
      "metadata": {
        "id": "9zUgPgyEZ2E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the vectors are stored in sparse format:\n",
        "train_vectors"
      ],
      "metadata": {
        "id": "_BaP1Jz43RR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to get the dense representation we need to convert them to an array:\n",
        "train_vectors.toarray()"
      ],
      "metadata": {
        "id": "a_0wi9r33wkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(30, 3))\n",
        "plt.plot(vectorizer.get_feature_names_out(), train_vectors.toarray()[0])\n",
        "plt.plot(vectorizer.get_feature_names_out(), train_vectors.toarray()[1])\n",
        "plt.plot(vectorizer.get_feature_names_out(), train_vectors.toarray()[2])\n",
        "plt.xticks(rotation=45, horizontalalignment='right');\n",
        "plt.ylim(0,1);"
      ],
      "metadata": {
        "id": "QZaVyRRK0T7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the text data to TF-IDF vectors, all words, all data\n",
        "vectorizer = ?\n",
        "train_vectors = ?\n",
        "test_vectors = ?"
      ],
      "metadata": {
        "id": "Jf4GAKPHtZSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what's the difference in processing train_vectors and test vectors?"
      ],
      "metadata": {
        "id": "KiuwscRaaYnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_vectors.toarray().shape"
      ],
      "metadata": {
        "id": "kqzoAV_9cH4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "af = train_vectors.toarray().flatten()\n",
        "# plot only present words (tfidf>0)\n",
        "aff = af[af>0]\n",
        "plt.hist(aff, 100, log=True);"
      ],
      "metadata": {
        "id": "IQTU4UaUuawJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Create and train the model"
      ],
      "metadata": {
        "id": "I4Zi-tyR5tNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# complete the code and train the model (20 min)"
      ],
      "metadata": {
        "id": "W3bn4Nn9a9vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oyrbUfDpqQX"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the model hyperparameters\n",
        "input_dim = train_vectors.shape[1]\n",
        "hidden_dim = 256\n",
        "output_dim = len(categories)\n",
        "\n",
        "# Create the model instance\n",
        "model = ?\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = ?\n",
        "criterion = ?\n",
        "\n",
        "# Set the device (GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Move the model and data to the device\n",
        "model = model.to(device)\n",
        "train_vectors_t = torch.tensor(train_vectors.toarray(), dtype=torch.float).to(device)\n",
        "train_labels_t = torch.tensor(train_data.target, dtype=torch.long).to(device)\n",
        "test_vectors_t = torch.tensor(test_vectors.toarray(), dtype=torch.float).to(device)\n",
        "test_labels_t = torch.tensor(test_data.target, dtype=torch.long).to(device)\n",
        "\n",
        "# Training loop\n",
        "def train(model, optimizer, criterion):\n",
        "    # set model to train mode\n",
        "    ?\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(train_vectors_t)\n",
        "    loss = criterion(predictions, train_labels_t)\n",
        "\n",
        "    # perform backpropagation\n",
        "    ?\n",
        "    optimizer.step()\n",
        "\n",
        "# Evaluation loop\n",
        "def evaluate(model):\n",
        "    # set model to evaluation mode\n",
        "    ?\n",
        "    with torch.no_grad():\n",
        "        predictions = model(test_vectors_t)\n",
        "        predicted_labels = torch.argmax(predictions, dim=1)\n",
        "        accuracy = torch.sum(predicted_labels == test_labels_t).item() / len(test_labels_t)\n",
        "    return accuracy\n",
        "\n",
        "# Training and evaluation loop\n",
        "NUM_EPOCHS = 100\n",
        "accuracy_arr = []\n",
        "for epoch in tqdm(range(NUM_EPOCHS)):\n",
        "    train(model, optimizer, criterion)\n",
        "    accuracy = evaluate(model)\n",
        "    #print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    accuracy_arr.append(accuracy)\n",
        "\n",
        "# plot the accuracy evolution\n",
        "?\n",
        "print(max(accuracy_arr))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What do you see?"
      ],
      "metadata": {
        "id": "66KqcGtlenZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Exercise 1h + 30min discussion"
      ],
      "metadata": {
        "id": "G-NUgYrD6Dfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Work in 3 groups:\n",
        " - present final group results in the end (10 min per group):\n",
        " - half time through - share code of your intermediate results (on Zoom) so that other groups use it for final results:\n",
        "\n",
        "1. Optimize the model architecture to improve performance.\n",
        "2. Study dependence of performance & training time on number of tf-idf features\n",
        "3. Study relevant performance metrics. Make evaluation code for both training and the test sets."
      ],
      "metadata": {
        "id": "RO89XTV16grj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtwfrYsDoKVX"
      },
      "source": [
        "# Wed morning 2: RNN on embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Imports/utils"
      ],
      "metadata": {
        "id": "wj0VGjy38y_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as gd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "import re"
      ],
      "metadata": {
        "id": "U_lEH0fEE5Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import tqdm\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "vinmxOLyyTS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "nI-xF3SBy9Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pckl(file_name, path=None):\n",
        "    if path is not None:\n",
        "        file_name = os.path.join(path, file_name)\n",
        "\n",
        "    with open(file_name, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_pckl(d, file_name, pr=None, path=None):\n",
        "    if path is not None:\n",
        "        file_name = os.path.join(path, file_name)\n",
        "\n",
        "    with open(file_name, 'wb') as f:\n",
        "        pickle.dump(d, f, protocol=pr if pr is not None else pickle.DEFAULT_PROTOCOL)"
      ],
      "metadata": {
        "id": "m1DUCo_g2x8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(input_string):\n",
        "    # Use regular expression to remove all punctuation characters\n",
        "    return re.sub(r'[^\\w\\s]', '', input_string)  # everything which is not a word (\\w) or space (\\s) -> empty string ('')"
      ],
      "metadata": {
        "id": "4PpKyLF7F9Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load and preprocess the dataset"
      ],
      "metadata": {
        "id": "XX6Kx_zwBb8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the 20 Newsgroups dataset\n",
        "categories = ['rec.sport.hockey', 'sci.electronics', 'comp.graphics']\n",
        "SEED = 64\n",
        "train_data = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=SEED, remove=('headers', 'footers', 'quotes'))\n",
        "test_data = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=SEED, remove=('headers', 'footers', 'quotes'))"
      ],
      "metadata": {
        "id": "ZW6zkt3LB47r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the train_data and test_data\n",
        "for d in [train_data, test_data]:\n",
        "\n",
        "  # Remove leading and trailing whitespace, tab,\n",
        "  # and new line characters from each data point\n",
        "  d.data = [s.strip(' \\n\\t\\r') for s in d.data]\n",
        "\n",
        "  # Get the indices of data points with non-empty content\n",
        "  ok_idx = [i for i, s in enumerate(d.data) if len(s) > 0]\n",
        "\n",
        "  # Filter the data and target lists to keep only non-empty data points\n",
        "  d.data = [d.data[i] for i in ok_idx]\n",
        "  d.target = [d.target[i] for i in ok_idx]"
      ],
      "metadata": {
        "id": "7HsPnlUEB47t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Explore Word2Vec (Homework)"
      ],
      "metadata": {
        "id": "EV15Ei_70NH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the tiny dataset example explore similar words and word arythmetic.\n",
        "\n",
        "Use your imagination, e.g. snake-long+short, or brick-hard+soft, etc"
      ],
      "metadata": {
        "id": "sO_Dqnvf_3UT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Tiny dataset"
      ],
      "metadata": {
        "id": "cAafXxdv_XNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the corpus (replace with your own preprocessing steps)\n",
        "corpus = [\"I love to eat pizza\", \"I hate Mondays\", \"Pizza is delicious\", \"I enjoy playing tennis\"]\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4, )"
      ],
      "metadata": {
        "id": "74dRLkXQ-iux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find similar words\n",
        "similar_words = model.wv.most_similar(\"pizza\")\n",
        "print(\"Similar words to 'pizza':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(word, similarity)"
      ],
      "metadata": {
        "id": "k9RvdcdY-kxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform word embeddings arithmetic\n",
        "result = model.wv.most_similar(positive=[\"tennis\", \"hate\"], negative=[\"love\"])  # \"tenis\" - \"love\" + \"hate\"\n",
        "print(\"Word embeddings arithmetic:\")\n",
        "for word, similarity in result:\n",
        "    print(word, similarity)\n"
      ],
      "metadata": {
        "id": "SbIZeU2e-qkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see - on tiny dataset the vectors don't make much sense.\n",
        "You are encouraged to explore the embedding vectors built based on the bigger datasets in the nxt 2 sections"
      ],
      "metadata": {
        "id": "3rQIG8CgBG5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Bigger dataset"
      ],
      "metadata": {
        "id": "oC5fQSXhJwMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the corpus (replace with your own preprocessing steps)\n",
        "corpus = train_data.data\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokenized_corpus = [remove_punctuation(sentence.lower()).split() for sentence in corpus]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(tokenized_corpus, vector_size=300, window=5, min_count=1, workers=4, )"
      ],
      "metadata": {
        "id": "nCwp9tAdE3m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"network\"\n",
        "similar_words = model.wv.most_similar(word, topn=20)\n",
        "print(f\"Similar words to '{word}':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(word, similarity)"
      ],
      "metadata": {
        "id": "kRjoibFZFBsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try removing the stopwords and re-fitting the model (see sections below hoe to do it)\n",
        "\n"
      ],
      "metadata": {
        "id": "tqACxydJCsKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 300k words"
      ],
      "metadata": {
        "id": "9JMEoreu_hly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here a pretrained model on many news articles is downloaded (Downloading it can take a while!). Feature vectors are of length 300, total 300k words."
      ],
      "metadata": {
        "id": "YoP9nc63DKUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the word2vec-google-news-300 model\n",
        "model_gn300 = gd.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "RhiLvBooJzcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_gn300.vectors.shape"
      ],
      "metadata": {
        "id": "9kilDS5znQ2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"car\"\n",
        "similar_words = model_gn300.most_similar(word)\n",
        "print(f\"Similar words to '{word}':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(word, similarity)"
      ],
      "metadata": {
        "id": "ZI_Dau6jJ254"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"god\"\n",
        "similar_words = model_gn300.most_similar(word)\n",
        "print(f\"Similar words to '{word}':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(word, similarity)"
      ],
      "metadata": {
        "id": "IklZGfZ1Keq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Find similar words\n",
        "similar_words = model_gn300.most_similar(\"python\")\n",
        "print(\"Similar words to 'pizza':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(word, similarity)\n",
        "\n",
        "# Perform word embeddings arithmetic\n",
        "result = model_gn300.most_similar(positive=[\"python\", \"young\"], negative=[\"old\"])\n",
        "print(\"Word embeddings arithmetic:\")\n",
        "for word, similarity in result:\n",
        "    print(word, similarity)\n",
        "\n",
        "# Visualize word embeddings using t-SNE (replace with your own visualization code)\n",
        "# ...\n",
        "\n",
        "# Additional tasks and experiments with word embeddings\n",
        "# ...\n"
      ],
      "metadata": {
        "id": "taBzRcUYEqSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Preparing embedding dataset (not run in the class - time-consuming)"
      ],
      "metadata": {
        "id": "sHqmaxTw7Myf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Embedding utilities"
      ],
      "metadata": {
        "id": "apoYWHguBV9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we create word embeddigns with Word2Vec"
      ],
      "metadata": {
        "id": "-BHATKtd8xc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download (takes a while!) the word2vec-google-news-300 model\n",
        "model_gn300 = gd.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "Z4THcnUGGahs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vec(model, word):\n",
        "  # convert word to vector\n",
        "  # Check if the model has an index for the word, of so get vector,\n",
        "  # otherwise return None\n",
        "  return model.get_vector(word) if model.has_index_for(word) else None"
      ],
      "metadata": {
        "id": "pq-cVuxRxMKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert text to word vectors\n",
        "# using a specified word embedding model\n",
        "# (default - word2vec-google-news-300 model, Download takes a while!\n",
        "# you can also try yours)\n",
        "def convert_text_to_vecs(text, model=model_gn300):\n",
        "  # Tokenize the input text into individual words\n",
        "  words = nltk.tokenize.word_tokenize(text)\n",
        "\n",
        "  # Get word vectors for each word in the text using the specified word embedding model\n",
        "  wvs = [get_vec(model, word) for word in words]\n",
        "\n",
        "  # Filter out None values (word vectors that couldn't be found in the model)\n",
        "  wvs = [v for v in wvs if v is not None]\n",
        "\n",
        "  # Convert the list of word vectors to a NumPy array\n",
        "  wvs = np.array(wvs)\n",
        "\n",
        "  # Return the array of word vectors for the input text\n",
        "  return wvs"
      ],
      "metadata": {
        "id": "f32YKff7x4Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few helper functions to convert input dataset, and to save whole dataset to file"
      ],
      "metadata": {
        "id": "eceVSYzTA6uQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_corpus_to_vecs(corpus, convert_corpus_to_vecs_fn):\n",
        "  # for each text in the corpus - vectorize it\n",
        "  # using list comprehension. Use tqdm to display progress\n",
        "  return [convert_corpus_to_vecs_fn(text) for text in tqdm.auto.tqdm(corpus)]"
      ],
      "metadata": {
        "id": "YGUcVK3Fx_rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_embedding_dataset(tra_input, tra_labels,\n",
        "                              val_input, val_labels,\n",
        "                              filename,\n",
        "                              convert_text_to_vecs_fn):\n",
        "  # Prepare and save an embedding dataset for training and validation.\n",
        "  print('embedding training data:')\n",
        "  tra_data_vecs = convert_corpus_to_vecs(tra_input, convert_text_to_vecs_fn)\n",
        "\n",
        "  print('embedding validation data:')\n",
        "  val_data_vecs = convert_corpus_to_vecs(val_input, convert_text_to_vecs_fn)\n",
        "\n",
        "  # remove empty elements\n",
        "  ok_idx = [i for i, s in enumerate(tra_data_vecs) if len(s)>0]\n",
        "  tra_data_vecs = [tra_data_vecs[i] for i in ok_idx]\n",
        "  tra_labels = [tra_labels[i] for i in ok_idx]\n",
        "\n",
        "  ok_idx = [i for i, s in enumerate(val_data_vecs) if len(s)>0]\n",
        "  val_data_vecs = [val_data_vecs[i] for i in ok_idx]\n",
        "  val_labels = [val_labels[i] for i in ok_idx]\n",
        "\n",
        "  print(f'preparing and saving dataset to: {filename}')\n",
        "\n",
        "  # Create a dataset dictionary containing training and validation data and labels\n",
        "  dataset = {\n",
        "    'tra_data': tra_data_vecs,\n",
        "    'tra_labels': tra_labels,\n",
        "    'val_data': val_data_vecs,\n",
        "    'val_labels': val_labels,\n",
        "  }\n",
        "\n",
        "  # Save the dataset as a pickle file\n",
        "  save_pckl(dataset, filename, path='./')"
      ],
      "metadata": {
        "id": "EolcP7t417Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe and RoBERTa embeddings (with flair)"
      ],
      "metadata": {
        "id": "6B65X6TPzvgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings, TransformerWordEmbeddings\n",
        "from flair.data import Sentence"
      ],
      "metadata": {
        "id": "T-PYAb2tDtKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download models for word embeddings with GloVe and RoBERTa\n",
        "glove_embedding = WordEmbeddings('glove')\n",
        "roberta_embedding = TransformerWordEmbeddings('roberta-base')\n",
        "\n",
        "# Create a DocumentPoolEmbeddings object (optional but helpful)\n",
        "document_embeddings_glove = DocumentPoolEmbeddings([glove_embedding])\n",
        "document_embeddings_roberta = DocumentPoolEmbeddings([roberta_embedding])\n"
      ],
      "metadata": {
        "id": "ukDDr4132om0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_text_to_embedding(text, doc_embedding_model):\n",
        "  # Create a Flair Sentence object\n",
        "  sentence = Sentence(text)\n",
        "\n",
        "  # Embed the sentence\n",
        "  try:\n",
        "    doc_embedding_model.embed(sentence)\n",
        "  except Exception as e:\n",
        "    # if model can't convert a text - print it, before raising the exception\n",
        "    print('failed text:', text)\n",
        "    raise e\n",
        "\n",
        "  embeddings = [token.embedding.cpu().numpy() for token in sentence]\n",
        "  return np.array(embeddings)"
      ],
      "metadata": {
        "id": "uA60Vu8F1giW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_text_to_glove(text):\n",
        "  return convert_text_to_embedding(text, document_embeddings_glove)\n",
        "\n",
        "def convert_text_to_roberta(text):\n",
        "  return convert_text_to_embedding(text, document_embeddings_roberta)\n"
      ],
      "metadata": {
        "id": "aNWc1s7g6BYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Create word2vec embeddings"
      ],
      "metadata": {
        "id": "bi1QNGL5BNZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_embedding_dataset(train_data.data, train_data.target,\n",
        "                          test_data.data, test_data.target,\n",
        "                          filename='dataset_20newsgroups_3_cat.pckl',\n",
        "                          convert_text_to_vecs_fn=convert_text_to_vecs\n",
        "                          )"
      ],
      "metadata": {
        "id": "gtQxEiXzxxgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv dataset_20newsgroups_3_cat.pckl \"/content/drive/MyDrive/Colab Data/NLP_data\""
      ],
      "metadata": {
        "id": "p8Gw4tVw5yZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Create GloVe embeddings"
      ],
      "metadata": {
        "id": "ccbxW2q4BgA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_embedding_dataset(train_data.data, train_data.target,\n",
        "                          test_data.data, test_data.target,\n",
        "                          filename='dataset_20newsgroups_3_cat_gl.pckl',\n",
        "                          convert_text_to_vecs_fn=convert_text_to_glove\n",
        "                          )"
      ],
      "metadata": {
        "id": "E_nuPY_S1NxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv dataset_20newsgroups_3_cat_gl.pckl \"/content/drive/MyDrive/Colab Data/NLP_data\""
      ],
      "metadata": {
        "id": "KzmZdxTQ6u3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Create RoBERTa embeddings (run with a GPU)"
      ],
      "metadata": {
        "id": "CsTsJdNMBlhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_embedding_dataset(train_data.data, train_data.target,\n",
        "                          test_data.data, test_data.target,\n",
        "                          filename='dataset_20newsgroups_3_cat_rb.pckl',\n",
        "                          convert_text_to_vecs_fn=convert_text_to_roberta\n",
        "                          )"
      ],
      "metadata": {
        "id": "18U8TklyByWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv dataset_20newsgroups_3_cat_rb.pckl \"/content/drive/MyDrive/Colab Data/NLP_data\""
      ],
      "metadata": {
        "id": "6mqQkRqACkH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Inspect prepared datasets:"
      ],
      "metadata": {
        "id": "_NiyDVri7Wt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_20newsgroups_3_cat = load_pckl('dataset_20newsgroups_3_cat.pckl', '/content/drive/MyDrive/NLP_data')"
      ],
      "metadata": {
        "id": "R-dlLdY6dQEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tra_data = dataset_20newsgroups_3_cat['tra_data']\n",
        "tra_labels = dataset_20newsgroups_3_cat['tra_labels']\n",
        "val_data = dataset_20newsgroups_3_cat['val_data']\n",
        "val_labels = dataset_20newsgroups_3_cat['val_labels']"
      ],
      "metadata": {
        "id": "Wes5HklPddf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tra_data), len(tra_labels), len(val_data), len(val_labels)"
      ],
      "metadata": {
        "id": "vdkC_4W3doQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tra_data[200].shape"
      ],
      "metadata": {
        "id": "yUY8vO-7d2cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. LSTM Training:"
      ],
      "metadata": {
        "id": "CirttWtLCH3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NYzup5MWG714"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ggt9UedMCXGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pckl(file_name, path=None):\n",
        "    if path is not None:\n",
        "        file_name = os.path.join(path, file_name)\n",
        "\n",
        "    with open(file_name, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_pckl(d, file_name, pr=None, path=None):\n",
        "    if path is not None:\n",
        "        file_name = os.path.join(path, file_name)\n",
        "\n",
        "    with open(file_name, 'wb') as f:\n",
        "        pickle.dump(d, f, protocol=pr if pr is not None else pickle.DEFAULT_PROTOCOL)"
      ],
      "metadata": {
        "id": "7mTTIDjjCay0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_name = 'dataset_20newsgroups_3_cat.pckl'\n",
        "\n",
        "\n",
        "# Load the embedded 20 Newsgroups dataset\n",
        "dataset = load_pckl(ds_name,\n",
        "                    '/content/drive/MyDrive/NLP_data')\n",
        "tra_data = dataset['tra_data']\n",
        "tra_labels = dataset['tra_labels']\n",
        "val_data = dataset['val_data']\n",
        "val_labels = dataset['val_labels']\n",
        "\n",
        "ok_idx = [i for i, s in enumerate(tra_data) if len(s)>0]\n",
        "tra_data = [tra_data[i] for i in ok_idx]\n",
        "tra_labels = [tra_labels[i] for i in ok_idx]\n",
        "\n",
        "ok_idx = [i for i, s in enumerate(val_data) if len(s)>0]\n",
        "val_data = [val_data[i] for i in ok_idx]\n",
        "val_labels = [val_labels[i] for i in ok_idx]\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Sjh8Uw6GD2fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect preprocessing function and ensure you understand each line (10 min)"
      ],
      "metadata": {
        "id": "KNzFkb8sb09p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 50\n",
        "\n",
        "# make preprocessing function converting data to torch tensors\n",
        "def preprocess(data, labels):\n",
        "    # Make random crops of the sequences up to max_len length,\n",
        "    # Pad short sequences to ensure consistent sequence length\n",
        "    max_len = 256  # Maximum sequence length\n",
        "    lens = [len(d) for d in data]  # Get the length of each data point\n",
        "\n",
        "    # Calculate random offsets for padding\n",
        "    ofs = [np.random.randint(0, max(1, len_i - max_len)) for len_i in lens]\n",
        "\n",
        "    # Apply padding based on offsets\n",
        "    data = [d[o:o + max_len] for d, o in zip(data, ofs)]\n",
        "    max_len = max([len(d) for d in data])  # Update maximum sequence length after padding\n",
        "\n",
        "    # Pad sequences with zeros to match the maximum sequence length\n",
        "    # pad_width contains size of padding  left and righ in each dimension\n",
        "    data_padded = [np.pad(d, pad_width=((0, max_len - len(d)),\n",
        "                                        (0, 0))) for d in data]\n",
        "\n",
        "    # Convert data and labels to NumPy arrays\n",
        "    data_padded = np.array(data_padded)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Convert NumPy arrays to PyTorch tensors\n",
        "    # and move them to the specified device (e.g., GPU)\n",
        "    data_t = torch.tensor(data_padded, dtype=torch.float32).to(device)\n",
        "    labels_t = torch.tensor(labels, dtype=torch.int64).to(device)\n",
        "\n",
        "    return data_t, labels_t\n",
        "\n",
        "# make data loader\n",
        "\n",
        "train_loader = DataLoader(list(zip(tra_data, tra_labels)),\n",
        "                          batch_size=batch_size, shuffle=True,\n",
        "                          collate_fn=lambda x: preprocess(*zip(*x)))\n",
        "\n",
        "test_loader = DataLoader(list(zip(val_data, val_labels)),\n",
        "                         batch_size=batch_size, shuffle=True,\n",
        "                         collate_fn=lambda x: preprocess(*zip(*x)))"
      ],
      "metadata": {
        "id": "Dy0cDxC8E_uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# complete the code below (20 min)"
      ],
      "metadata": {
        "id": "StC8eJqUce_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationRNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(ClassificationRNN, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
        "\n",
        "        if type(hidden_dim) == int:\n",
        "          hidden_dim = [hidden_dim]\n",
        "\n",
        "        self.rnn = []\n",
        "        for i, hidd_d in enumerate(hidden_dim):\n",
        "          prev_d = embedding_dim if i == 0 else hidden_dim[i-1]\n",
        "          rnn = nn.LSTM(prev_d, hidd_d, batch_first=True)\n",
        "\n",
        "          # most important line:\n",
        "          self.add_module(f'lstm_{i}', rnn)\n",
        "          self.rnn.append(rnn)\n",
        "\n",
        "        self.fc = nn.Linear(hidd_d, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply embedding layer and relu to the input\n",
        "        embedded = ?\n",
        "\n",
        "        # apply each rnn layer. tip: what does each layer return?\n",
        "        for rnn in self.rnn:\n",
        "          ?? = ?\n",
        "\n",
        "        output = embedded\n",
        "        rnn_out = hidden_hn[0]  # hn\n",
        "        return self.fc(rnn_out)"
      ],
      "metadata": {
        "id": "jZo4-qLuEvKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "input_dim = tra_data[0].shape[1]\n",
        "embedding_dim = 256\n",
        "hidden_dim = [128, 16]  # you can specify a list of number of units in sequential LSTM layers\n",
        "output_dim = 3\n",
        "\n",
        "model = ClassificationRNN(input_dim, embedding_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "# Define the sparse cross-entropy loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.005)\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 160\n",
        "\n",
        "tra_loss_hist = []\n",
        "val_loss_hist = []\n",
        "val_acc_hist = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.\n",
        "    valid_loss = 0.\n",
        "\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        data, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # perform a training update:\n",
        "        ?\n",
        "\n",
        "        train_loss += loss.item()\n",
        "    train_loss /= len(train_loader)\n",
        "    tra_loss_hist.append(train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = []\n",
        "        for batch in test_loader:\n",
        "            data, labels = batch\n",
        "            output = model(data)\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            pred_class = torch.argmax(output, dim=1)\n",
        "            corr = pred_class == labels\n",
        "            correct.append(corr.detach().cpu().numpy())\n",
        "\n",
        "        valid_loss /= len(test_loader)\n",
        "        correct = np.concatenate(correct)\n",
        "        accuracy = np.mean(correct)\n",
        "        print(f\"{epoch}:\\t Test loss: {valid_loss}; accuracy: {accuracy}\")\n",
        "\n",
        "        val_loss_hist.append(valid_loss)\n",
        "        val_acc_hist.append(accuracy)"
      ],
      "metadata": {
        "id": "KUgZVU4LIS2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss and accuracy on 2 subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "ax1.plot(tra_loss_hist, label='train')\n",
        "ax1.plot(val_loss_hist, label='test')\n",
        "ax1.set_xlabel('epoch')\n",
        "ax1.set_ylabel('loss')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(val_acc_hist)\n",
        "ax2.set_xlabel('epoch')\n",
        "ax2.set_ylabel('accuracy')\n",
        "plt.show()\n",
        "\n",
        "print(f'best validation accuracy: {np.max(val_acc_hist)} @ epoch {np.argmax(val_acc_hist)}')"
      ],
      "metadata": {
        "id": "MdKYzdosJsXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Exercise 1."
      ],
      "metadata": {
        "id": "ay-9UVS3I8AL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Work in 3 groups, use the 3 datasets.\n",
        "Optimize the model architecture: # layers, units per layer (hidden_dim) and the embedding_dim to improve validation accuracy."
      ],
      "metadata": {
        "id": "QWVq4Yp1I_v4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Tip"
      ],
      "metadata": {
        "id": "zQ7YqgUjJpgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same Flair interface as for Glove and Roberta, can be used to concatenate embeddings from several models, including your own ones. E.g., given your model file 'my_model.pt', you can create document embedder:\n",
        "\n",
        "```\n",
        "my_model = torch.load('my_model.pt')\n",
        "document_embeddings_my_model = DocumentPoolEmbeddings([my_model])\n",
        "```"
      ],
      "metadata": {
        "id": "oIHJx96bJpgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "alternatively - you can combine the Glove embeddings and your emebedding, or the roberta ones:\n",
        "```\n",
        "document_embeddings_stacked = DocumentPoolEmbeddings([my_model, glove_embedding])\n",
        "```"
      ],
      "metadata": {
        "id": "jfKeQJujMSO2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4SFDHxitoA-5",
        "p1nUMIbUoLl-",
        "AtwfrYsDoKVX",
        "wj0VGjy38y_d",
        "XX6Kx_zwBb8e",
        "EV15Ei_70NH0",
        "sHqmaxTw7Myf",
        "CirttWtLCH3T"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}